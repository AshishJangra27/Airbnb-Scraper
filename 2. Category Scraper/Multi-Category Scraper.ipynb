{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e38f8bb0",
   "metadata": {},
   "source": [
    "#### 1. Loading the Libraries and WebDriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2bc0880",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "\n",
    "browser = webdriver.Chrome('/Users/ashishzangra/opt/anaconda3/lib/python3.9/site-packages/chromedriver_binary/chromedriver')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e626bd",
   "metadata": {},
   "source": [
    "#### 2. Opening the Website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c07e245",
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.get('https://www.airbnb.co.in/')\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3f6f27",
   "metadata": {},
   "source": [
    "#### 3. Defining the Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899cec1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'https://www.airbnb.co.in/?tab_id=home_tab&refinement_paths%5B%5D=%2Fhomes&search_mode=flex_destinations_search&flexible_trip_lengths%5B%5D=one_week&location_search=MIN_MAP_BOUNDS&search_type=category_change&category_tag=Tag%'\n",
    "\n",
    "cateogories = { 'OMG!'             : '3A8225', 'National parks'     : '3A8102', 'Tiny homes'        : '3A8186',\n",
    "                'Islands'          : '3A675' , 'Camping'            : '3A634' , 'Cabins'            : '3A5348',\n",
    "                'Design'           : '3A8528', 'Arctic'             : '3A8534', 'Amazing pools'     : '3A677' ,\n",
    "                'Lakefront'        : '3A8522', 'Surfing'            : '3A8526', 'A-frames'          : '3A8148', \n",
    "                'Treehouses'       : '3A8188', 'Tropical'           : '3A5635', 'Bed & breakfasts'  : '3A8538', \n",
    "                'Caves'            : '3A670' , 'Shared homes'       : '3A8542', 'Earth homes'       : '3A8174', \n",
    "                'Farms'            : '3A8175', 'Countryside'        : '3A4104', 'Luxe'              : '3A8253', \n",
    "                'Golfing'          : 'A8525' , 'Amazing views'      : '3A8536', 'Castles'           : '3A8047', \n",
    "                'Iconic cities'    : '3A8535', 'Historical homes'   : 'A8524' , 'Mansions'          : '3A8115', \n",
    "                'Beaches'          : '3A7769', 'Cycladic homes'     : '3A8227', 'Domes'             : '3A8173', \n",
    "                'Campervans'       : '3A8166', \"Chef's kitchens\"    : '3A5731', 'Windmills'         : '3A8043', \n",
    "                'Vineyards'        : '3A8101', 'Casas particulares' : '3A8232', 'Off-the-grid'      : '3A8226', \n",
    "                'Skiing'           : '3A7765', 'Minsus'             : '3A8230', 'Yurts'             : '3A8192', \n",
    "                'Desert'           : '3A8099', 'Ryokans'            : '3A8228', 'Towers'            : '3A8187', \n",
    "                \"Shepherd's huts\"  : '3A8650', 'Barns'              : '3A8159', 'Houseboats'        : '3A8176', \n",
    "                'Boats'            : '3A1073', 'Beachfront'         : '3A789' , 'Containers'        : '3A8157', \n",
    "                'Grand pianos'     : '3A8521', 'Creative spaces'    : '3A8543', 'Trulli'            : '3A8229', \n",
    "                'Riads'            : '3A8255', 'Dammusos'           : '3A8256', 'Ski-in-Ski-out'    : '3A5366', \n",
    "                'Lake'             : '3A8144'}\n",
    "\n",
    "print(len(cateogories))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6ddbc9",
   "metadata": {},
   "source": [
    "#### 4. Scraping the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214b9755",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in tqdm(list(cateogories.keys())):\n",
    "    \n",
    "    link = prefix + cateogories[key]\n",
    "\n",
    "    # Send Request on Each Link\n",
    "    browser.get(link)\n",
    "    time.sleep(5)\n",
    "    soup = BeautifulSoup(browser.page_source, 'html.parser')\n",
    "    \n",
    "\n",
    "    # Reach the bottom of the Page\n",
    "    i = 0\n",
    "    no_of_rest_c = 1\n",
    "    no_of_rest_o = 0\n",
    "    while True:\n",
    "        browser.execute_script(\"window.scrollTo(0,\" + str(i) + \")\")\n",
    "        i += 100\n",
    "        time.sleep(.2)\n",
    "        if (i%10000 == 0):\n",
    "            soup = BeautifulSoup(browser.page_source, 'html.parser')\n",
    "            no_of_rest_c = len(soup.find_all('div', class_ = 'c4mnd7m dir dir-ltr'))\n",
    "            if (no_of_rest_o == no_of_rest_c):\n",
    "                break\n",
    "            no_of_rest_o = no_of_rest_c\n",
    "            \n",
    "            \n",
    "    # Scraping the Details of the stay        \n",
    "    data = []\n",
    "    for sp in soup.find_all('div', class_ = 'c4mnd7m dir dir-ltr'):\n",
    "        try:\n",
    "            img_link = sp.find('img').get('src')\n",
    "        except:\n",
    "            img_link = np.nan\n",
    "        try:\n",
    "            id_      = sp.find('div', class_ = 'g1tup9az cb4nyux dir dir-ltr').find_all('div')[0].get('id').strip()[6:]\n",
    "        except:\n",
    "            id_      = np.nan\n",
    "        try:\n",
    "            name     = sp.find('div', class_ = 'g1tup9az cb4nyux dir dir-ltr').find_all('div')[0].text.strip()\n",
    "        except:\n",
    "            name     = np.nan\n",
    "        try:\n",
    "            price    = sp.find('div', class_ = 'g1tup9az cb4nyux dir dir-ltr').find_all('span')[7].text.strip()\n",
    "        except:\n",
    "            price    = np.nan\n",
    "        try:\n",
    "            rating   = sp.find('div', class_ = 'g1tup9az cb4nyux dir dir-ltr').find_all('span')[-1].text.strip()\n",
    "        except:\n",
    "            rating   = np.nan\n",
    "        data.append([id_, name, price, rating, img_link])\n",
    "    \n",
    "    # Saving the Dataset\n",
    "    df = pd.DataFrame(data, columns = ['id', 'name','price','rating','img_link'])\n",
    "    df.to_csv(key + '.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67262de",
   "metadata": {},
   "source": [
    "#### 5. Finding the Incomplete Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4788aa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for file in os.listdir('Datasets'):\n",
    "    \n",
    "    if '.csv' in file:                                    # Verifying the CSV Files\n",
    "        \n",
    "        df_ = pd.read_csv('Datasets/' + file)             # Loading each CSV file one by one\n",
    "        \n",
    "        name = file.split('.')[0]                         # Taking the name of the CSV file\n",
    "        \n",
    "        if (len(df_) == 0 or df_.isnull().sum()[0]):      # Need to Scraper Again\n",
    "            print(name, '<!!!>')\n",
    "            \n",
    "        else:                                             # Scraped\n",
    "            del cateogories[name]                         # Removing the Pairs whose data is Scraped\n",
    "        \n",
    "print(len(cateogories))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1166fd",
   "metadata": {},
   "source": [
    "#### 6. Scraping Incomplete Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d411c1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in tqdm(list(cateogories.keys())):\n",
    "    \n",
    "    link = prefix + cateogories[key]\n",
    "\n",
    "    # Send Request on Each Link\n",
    "    browser.get(link)\n",
    "    time.sleep(5)\n",
    "    soup = BeautifulSoup(browser.page_source, 'html.parser')\n",
    "\n",
    "\n",
    "    # Reach the bottom of the Page\n",
    "    i = 0\n",
    "    no_of_rest_c = 1\n",
    "    no_of_rest_o = 0\n",
    "    while True:\n",
    "        browser.execute_script(\"window.scrollTo(0,\" + str(i) + \")\")\n",
    "        i += 100\n",
    "        time.sleep(.2)\n",
    "        if (i%10000 == 0):\n",
    "            soup = BeautifulSoup(browser.page_source, 'html.parser')\n",
    "            no_of_rest_c = len(soup.find_all('div', class_ = 'c4mnd7m dir dir-ltr'))\n",
    "            if (no_of_rest_o == no_of_rest_c):\n",
    "                break\n",
    "            no_of_rest_o = no_of_rest_c\n",
    "            \n",
    "            \n",
    "    # Scraping the Details of the stay        \n",
    "    data = []\n",
    "    for sp in soup.find_all('div', class_ = 'c4mnd7m dir dir-ltr'):\n",
    "        try:\n",
    "            img_link = sp.find('img').get('src')\n",
    "        except:\n",
    "            img_link = np.nan\n",
    "        try:\n",
    "            id_      = sp.find('div', class_ = 'g1tup9az cb4nyux dir dir-ltr').find_all('div')[0].get('id').strip()[6:]\n",
    "        except:\n",
    "            id_      = np.nan\n",
    "        try:\n",
    "            name     = sp.find('div', class_ = 'g1tup9az cb4nyux dir dir-ltr').find_all('div')[0].text.strip()\n",
    "        except:\n",
    "            name     = np.nan\n",
    "        try:\n",
    "            price    = sp.find('div', class_ = 'g1tup9az cb4nyux dir dir-ltr').find_all('span')[7].text.strip()\n",
    "        except:\n",
    "            price    = np.nan\n",
    "        try:\n",
    "            rating   = sp.find('div', class_ = 'g1tup9az cb4nyux dir dir-ltr').find_all('span')[-1].text.strip()\n",
    "        except:\n",
    "            rating   = np.nan\n",
    "        data.append([id_, name, price, rating, img_link])\n",
    "    \n",
    "    # Saving the Dataset\n",
    "    df = pd.DataFrame(data, columns = ['id', 'name','price','rating','img_link'])\n",
    "    df.to_csv('Datasets/' + key + '.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d653d1",
   "metadata": {},
   "source": [
    "#### 7. Automatic Scraper untill the whole data is scraped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f6ef34",
   "metadata": {},
   "outputs": [],
   "source": [
    "while (len(cateogories) != 0):\n",
    "\n",
    "    # 1. Finding the Files that need to be scraped Again\n",
    "\n",
    "    for file in os.listdir('Datasets'):   \n",
    "        if '.csv' in file:                                    # Verifying the CSV Files\n",
    "            df_ = pd.read_csv('Datasets/' + file)             # Loading each CSV file one by one \n",
    "            name = file.split('.')[0]                         # Taking the name of the CSV file\n",
    "            if (len(df_) == 0 or df_.isnull().sum()[0]):      # Need to Scraper Again\n",
    "                pass\n",
    "#                 print(name, '<!!!>')  \n",
    "            else:                                             # Scraped\n",
    "                try:\n",
    "                    del cateogories[name]                         # Removing the Pairs whose data is Scraped\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "\n",
    "    # 2. Scraping the Incomplete Data Again            \n",
    "\n",
    "    for key in tqdm(list(cateogories.keys())):\n",
    "\n",
    "        link = prefix + cateogories[key]\n",
    "\n",
    "        # Send Request on Each Link\n",
    "        browser.get(link)\n",
    "        time.sleep(5)\n",
    "        soup = BeautifulSoup(browser.page_source, 'html.parser')\n",
    "\n",
    "\n",
    "        # Reach the bottom of the Page\n",
    "        i = 0\n",
    "        no_of_rest_c = 1\n",
    "        no_of_rest_o = 0\n",
    "        while True:\n",
    "            browser.execute_script(\"window.scrollTo(0,\" + str(i) + \")\")\n",
    "            i += 100\n",
    "            time.sleep(.2)\n",
    "            if (i%10000 == 0):\n",
    "                soup = BeautifulSoup(browser.page_source, 'html.parser')\n",
    "                no_of_rest_c = len(soup.find_all('div', class_ = 'c4mnd7m dir dir-ltr'))\n",
    "                if (no_of_rest_o == no_of_rest_c):\n",
    "                    break\n",
    "                no_of_rest_o = no_of_rest_c\n",
    "\n",
    "\n",
    "        # Scraping the Details of the stay        \n",
    "        data = []\n",
    "        for sp in soup.find_all('div', class_ = 'c4mnd7m dir dir-ltr'):\n",
    "            try:\n",
    "                img_link = sp.find('img').get('src')\n",
    "            except:\n",
    "                img_link = np.nan\n",
    "            try:\n",
    "                id_      = sp.find('div', class_ = 'g1tup9az cb4nyux dir dir-ltr').find_all('div')[0].get('id').strip()[6:]\n",
    "            except:\n",
    "                id_      = np.nan\n",
    "            try:\n",
    "                name     = sp.find('div', class_ = 'g1tup9az cb4nyux dir dir-ltr').find_all('div')[0].text.strip()\n",
    "            except:\n",
    "                name     = np.nan\n",
    "            try:\n",
    "                price    = sp.find('div', class_ = 'g1tup9az cb4nyux dir dir-ltr').find_all('span')[7].text.strip()\n",
    "            except:\n",
    "                price    = np.nan\n",
    "            try:\n",
    "                rating   = sp.find('div', class_ = 'g1tup9az cb4nyux dir dir-ltr').find_all('span')[-1].text.strip()\n",
    "            except:\n",
    "                rating   = np.nan\n",
    "            data.append([id_, name, price, rating, img_link])\n",
    "\n",
    "        # Saving the Dataset\n",
    "        df = pd.DataFrame(data, columns = ['id', 'name','price','rating','img_link'])\n",
    "        df.to_csv('Datasets/' + key + '.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d890d8",
   "metadata": {},
   "source": [
    "### 8. Data Combining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5360e342",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_master = pd.DataFrame()\n",
    "\n",
    "for file in os.listdir('Datasets/'):\n",
    "    \n",
    "    if ('csv' in file):\n",
    "        \n",
    "        df_ = pd.read_csv('Datasets/' + file)\n",
    "        df_['category'] = file.split('.')[0]\n",
    "        \n",
    "        df_master = pd.concat((df_master, df_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b8b3be",
   "metadata": {},
   "source": [
    "### 9. Saving the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46c04dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_master.to_csv('data.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
